{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II : Logistic Regression Using Gradient Descent\n",
    "\n",
    "### Dataset : Penguins Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read preprocessed penguins dataset\n",
    "penguins_data = pd.read_csv('./preprocessed/penguins_preprocessed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>species_adelie</th>\n",
       "      <th>species_chinstrap</th>\n",
       "      <th>species_gentoo</th>\n",
       "      <th>island_biscoe</th>\n",
       "      <th>island_dream</th>\n",
       "      <th>island_na</th>\n",
       "      <th>island_torgersen</th>\n",
       "      <th>gender_female</th>\n",
       "      <th>gender_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>190.0</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>55.8</td>\n",
       "      <td>19.8</td>\n",
       "      <td>207.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>43.5</td>\n",
       "      <td>18.1</td>\n",
       "      <td>202.0</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>49.6</td>\n",
       "      <td>18.2</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>50.8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>50.2</td>\n",
       "      <td>18.7</td>\n",
       "      <td>198.0</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
       "0              39.1           18.7              181.0       3750.0   \n",
       "1              39.5           17.4              186.0       3800.0   \n",
       "2              40.3           18.0              195.0       3250.0   \n",
       "3              36.7           19.3              193.0       3450.0   \n",
       "4              39.3           20.6              190.0       3650.0   \n",
       "..              ...            ...                ...          ...   \n",
       "311            55.8           19.8              207.0       4000.0   \n",
       "312            43.5           18.1              202.0       3400.0   \n",
       "313            49.6           18.2              193.0       3775.0   \n",
       "314            50.8           19.0              210.0       4100.0   \n",
       "315            50.2           18.7              198.0       3775.0   \n",
       "\n",
       "     species_adelie  species_chinstrap  species_gentoo  island_biscoe  \\\n",
       "0                 1                  0               0              0   \n",
       "1                 1                  0               0              0   \n",
       "2                 1                  0               0              0   \n",
       "3                 1                  0               0              0   \n",
       "4                 1                  0               0              0   \n",
       "..              ...                ...             ...            ...   \n",
       "311               0                  1               0              0   \n",
       "312               0                  1               0              0   \n",
       "313               0                  1               0              0   \n",
       "314               0                  1               0              0   \n",
       "315               0                  1               0              0   \n",
       "\n",
       "     island_dream  island_na  island_torgersen  gender_female  gender_male  \n",
       "0               0          0                 1              0            1  \n",
       "1               0          0                 1              1            0  \n",
       "2               0          0                 1              1            0  \n",
       "3               0          0                 1              1            0  \n",
       "4               0          0                 1              0            1  \n",
       "..            ...        ...               ...            ...          ...  \n",
       "311             1          0                 0              0            1  \n",
       "312             1          0                 0              1            0  \n",
       "313             1          0                 0              0            1  \n",
       "314             1          0                 0              0            1  \n",
       "315             1          0                 0              1            0  \n",
       "\n",
       "[316 rows x 13 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penguins_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Choosing target Y and split the dataset\n",
    "\n",
    "- Here in this case I choose penguin gender (male or female) as the target variable Y. So we need to predict whether a penguin is male or female - a Classification problem.\n",
    "- I can just drop the 'gender_female' column for simplicity. The model output will be whether the input penguin is Male or Not\n",
    "- In other words, gender(penguin) == Male, return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shapes:  (253, 12)\n",
      "test data shapes:  (63, 12)\n"
     ]
    }
   ],
   "source": [
    "copy_final = penguins_data.drop('gender_female', axis=1)\n",
    "\n",
    "# Creating training and test set using 80% train and 20% test ratio\n",
    "training_set = copy_final.sample(frac=0.8, random_state=128) \n",
    "test_set = copy_final.drop(training_set.index)\n",
    "\n",
    "print(\"train data shapes: \", training_set.shape)\n",
    "print(\"test data shapes: \", test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(column):\n",
    "    return (column - column.min()) / (column.max() - column.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>species_adelie</th>\n",
       "      <th>species_chinstrap</th>\n",
       "      <th>species_gentoo</th>\n",
       "      <th>island_biscoe</th>\n",
       "      <th>island_dream</th>\n",
       "      <th>island_na</th>\n",
       "      <th>island_torgersen</th>\n",
       "      <th>gender_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.080595</td>\n",
       "      <td>0.035915</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.071916</td>\n",
       "      <td>0.048178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.245505</td>\n",
       "      <td>0.056062</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.123993</td>\n",
       "      <td>0.052558</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.084315</td>\n",
       "      <td>0.041170</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
       "95         0.080595       0.035915           0.457627     0.291667   \n",
       "21         0.071916       0.048178           0.000000     0.125000   \n",
       "308        0.245505       0.056062           0.576271     0.347222   \n",
       "69         0.123993       0.052558           0.389831     0.361111   \n",
       "5          0.084315       0.041170           0.152542     0.256944   \n",
       "\n",
       "     species_adelie  species_chinstrap  species_gentoo  island_biscoe  \\\n",
       "95                1                  0               0              1   \n",
       "21                1                  0               0              1   \n",
       "308               0                  1               0              0   \n",
       "69                1                  0               0              0   \n",
       "5                 1                  0               0              0   \n",
       "\n",
       "     island_dream  island_na  island_torgersen  gender_male  \n",
       "95              0          0                 0            0  \n",
       "21              0          0                 0            0  \n",
       "308             1          0                 0            1  \n",
       "69              0          0                 1            1  \n",
       "5               0          0                 1            0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize training set\n",
    "for column in ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']:\n",
    "    training_set[column] = normalize(training_set[column])\n",
    "\n",
    "training_set[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>species_adelie</th>\n",
       "      <th>species_chinstrap</th>\n",
       "      <th>species_gentoo</th>\n",
       "      <th>island_biscoe</th>\n",
       "      <th>island_dream</th>\n",
       "      <th>island_na</th>\n",
       "      <th>island_torgersen</th>\n",
       "      <th>gender_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.055982</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.274194</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.060849</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.025557</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.045029</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.134615</td>\n",
       "      <td>0.338710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.052331</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
       "0         0.055982       0.833333           0.057692     0.274194   \n",
       "1         0.060849       0.616667           0.153846     0.290323   \n",
       "10        0.025557       0.683333           0.134615     0.258065   \n",
       "16        0.045029       0.733333           0.134615     0.338710   \n",
       "17        0.052331       0.583333           0.038462     0.290323   \n",
       "\n",
       "    species_adelie  species_chinstrap  species_gentoo  island_biscoe  \\\n",
       "0                1                  0               0              0   \n",
       "1                1                  0               0              0   \n",
       "10               1                  0               0              0   \n",
       "16               1                  0               0              1   \n",
       "17               1                  0               0              1   \n",
       "\n",
       "    island_dream  island_na  island_torgersen  gender_male  \n",
       "0              0          0                 1            1  \n",
       "1              0          0                 1            0  \n",
       "10             0          0                 1            0  \n",
       "16             0          0                 0            1  \n",
       "17             0          0                 0            1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize test set\n",
    "for column in ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']:\n",
    "    test_set[column] = normalize(test_set[column])\n",
    "\n",
    "test_set[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shapes:  (253, 11) (253,)\n",
      "test data shapes:  (63, 11) (63,)\n"
     ]
    }
   ],
   "source": [
    "x_train = training_set.drop('gender_male', axis=1)\n",
    "y_train = training_set['gender_male']\n",
    "\n",
    "x_test = test_set.drop('gender_male', axis=1)\n",
    "y_test = test_set['gender_male']\n",
    "\n",
    "print(\"train data shapes: \", x_train.shape, y_train.shape)\n",
    "print(\"test data shapes: \", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitRegression():\n",
    "    def __init__(self, learning_rate, iterations):\n",
    "        self.alpha = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.weights = None\n",
    "        self.n = None\n",
    "        self.loss = []\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def cost(self, y, y_pred):\n",
    "        jw = (1/self.n)*np.sum(-y*np.log(y_pred) - (1-y)*np.log(1-y_pred))\n",
    "        return jw\n",
    "\n",
    "    def gradient_descent(self, X, y):\n",
    "        y_pred = self.sigmoid(np.dot(X, self.weights))\n",
    "        delta = y_pred - y\n",
    "        dw = np.dot(X.T, delta)/self.n\n",
    "        self.weights -= self.alpha*dw #Update Weights\n",
    "\n",
    "\n",
    "    def fit(self, X, y, w):\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            print(\"X and Y are not of same dimentions\")\n",
    "            return \n",
    "\n",
    "        self.n = X.shape[0]\n",
    "\n",
    "        # Initialize weights(W_0 for bias term)\n",
    "        self.weights = w\n",
    "        self.weights[0] = 1 #for bias term\n",
    "\n",
    "        # Add ones to starting row for bias term\n",
    "        X = np.column_stack((np.ones(X.shape[0]), X))\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            # Call Gradient Descent\n",
    "            self.gradient_descent(X, y)\n",
    "\n",
    "            y_pred = self.sigmoid(np.dot(X, self.weights))\n",
    "            loss = self.cost(y, y_pred)\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"Loss at iteration {i}: {loss}\")\n",
    "            self.loss.append(loss)\n",
    "\n",
    "        pickle.dump(self.weights, open(\"log_reg_weights.p\", \"wb\" ))\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.insert(X, 0, 1)\n",
    "        try:\n",
    "            wt = pickle.load(open(\"log_reg_weights.p\", \"rb\" ) )\n",
    "        except FileNotFoundError:\n",
    "            wt = self.weights\n",
    "        y_pred = self.sigmoid(np.dot(X, wt))\n",
    "        if y_pred >= 0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_model = LogitRegression(0.001, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random weights\n",
    "intial_w = np.random.rand(1 + x_train.shape[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "intial_w2 = np.ones(1 + x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "intial_w3 = np.zeros(1 + x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0: 1.3336261383341002\n",
      "Loss at iteration 1000: 1.0312253282908603\n",
      "Loss at iteration 2000: 0.8476627041202122\n",
      "Loss at iteration 3000: 0.75774783945615\n",
      "Loss at iteration 4000: 0.7190195715460993\n",
      "Loss at iteration 5000: 0.702179210524151\n",
      "Loss at iteration 6000: 0.6936875569966731\n",
      "Loss at iteration 7000: 0.6883279653317756\n",
      "Loss at iteration 8000: 0.6842173695421299\n",
      "Loss at iteration 9000: 0.6806763933386757\n",
      "Loss at iteration 10000: 0.6774470335796253\n",
      "Loss at iteration 11000: 0.6744201679257998\n",
      "Loss at iteration 12000: 0.6715418270137671\n",
      "Loss at iteration 13000: 0.6687805182309544\n",
      "Loss at iteration 14000: 0.6661153271731054\n",
      "Loss at iteration 15000: 0.6635311957707294\n",
      "Loss at iteration 16000: 0.6610167798809218\n",
      "Loss at iteration 17000: 0.6585633093024262\n",
      "Loss at iteration 18000: 0.656163889190952\n",
      "Loss at iteration 19000: 0.6538130285531435\n",
      "Loss at iteration 20000: 0.651506303805598\n",
      "Loss at iteration 21000: 0.6492401114453774\n",
      "Loss at iteration 22000: 0.6470114832272498\n",
      "Loss at iteration 23000: 0.6448179466544407\n",
      "Loss at iteration 24000: 0.642657418905371\n",
      "Loss at iteration 25000: 0.6405281256889935\n",
      "Loss at iteration 26000: 0.6384285388174258\n",
      "Loss at iteration 27000: 0.6363573279116996\n",
      "Loss at iteration 28000: 0.6343133228336052\n",
      "Loss at iteration 29000: 0.6322954842978603\n",
      "Loss at iteration 30000: 0.6303028807533851\n",
      "Loss at iteration 31000: 0.6283346700923402\n",
      "Loss at iteration 32000: 0.6263900850950612\n",
      "Loss at iteration 33000: 0.6244684217800155\n",
      "Loss at iteration 34000: 0.6225690300236252\n",
      "Loss at iteration 35000: 0.6206913059622015\n",
      "Loss at iteration 36000: 0.6188346857996988\n",
      "Loss at iteration 37000: 0.6169986407296962\n",
      "Loss at iteration 38000: 0.6151826727446089\n",
      "Loss at iteration 39000: 0.6133863111546668\n",
      "Loss at iteration 40000: 0.6116091096773062\n",
      "Loss at iteration 41000: 0.609850643987109\n",
      "Loss at iteration 42000: 0.6081105096392995\n",
      "Loss at iteration 43000: 0.6063883202976699\n",
      "Loss at iteration 44000: 0.6046837062117741\n",
      "Loss at iteration 45000: 0.6029963128992163\n",
      "Loss at iteration 46000: 0.6013257999975344\n",
      "Loss at iteration 47000: 0.5996718402570362\n",
      "Loss at iteration 48000: 0.5980341186514098\n",
      "Loss at iteration 49000: 0.5964123315872712\n",
      "Loss at iteration 50000: 0.5948061861972962\n",
      "Loss at iteration 51000: 0.5932153997043741\n",
      "Loss at iteration 52000: 0.5916396988464628\n",
      "Loss at iteration 53000: 0.5900788193536425\n",
      "Loss at iteration 54000: 0.5885325054703227\n",
      "Loss at iteration 55000: 0.5870005095167568\n",
      "Loss at iteration 56000: 0.5854825914849767\n",
      "Loss at iteration 57000: 0.5839785186650536\n",
      "Loss at iteration 58000: 0.5824880652982364\n",
      "Loss at iteration 59000: 0.5810110122540464\n",
      "Loss at iteration 60000: 0.5795471467288438\n",
      "Loss at iteration 61000: 0.5780962619637424\n",
      "Loss at iteration 62000: 0.5766581569800393\n",
      "Loss at iteration 63000: 0.575232636330585\n",
      "Loss at iteration 64000: 0.5738195098657142\n",
      "Loss at iteration 65000: 0.5724185925125347\n",
      "Loss at iteration 66000: 0.5710297040665178\n",
      "Loss at iteration 67000: 0.5696526689944515\n",
      "Loss at iteration 68000: 0.5682873162479258\n",
      "Loss at iteration 69000: 0.566933479086605\n",
      "Loss at iteration 70000: 0.5655909949106159\n",
      "Loss at iteration 71000: 0.5642597051014485\n",
      "Loss at iteration 72000: 0.5629394548708228\n",
      "Loss at iteration 73000: 0.5616300931170172\n",
      "Loss at iteration 74000: 0.5603314722882068\n",
      "Loss at iteration 75000: 0.5590434482523877\n",
      "Loss at iteration 76000: 0.5577658801735038\n",
      "Loss at iteration 77000: 0.5564986303934161\n",
      "Loss at iteration 78000: 0.5552415643193853\n",
      "Loss at iteration 79000: 0.5539945503167621\n",
      "Loss at iteration 80000: 0.5527574596065922\n",
      "Loss at iteration 81000: 0.5515301661678786\n",
      "Loss at iteration 82000: 0.5503125466442441\n",
      "Loss at iteration 83000: 0.5491044802547632\n",
      "Loss at iteration 84000: 0.5479058487087446\n",
      "Loss at iteration 85000: 0.5467165361242597\n",
      "Loss at iteration 86000: 0.54553642895022\n",
      "Loss at iteration 87000: 0.5443654158918282\n",
      "Loss at iteration 88000: 0.543203387839225\n",
      "Loss at iteration 89000: 0.5420502377991763\n",
      "Loss at iteration 90000: 0.5409058608296446\n",
      "Loss at iteration 91000: 0.5397701539771039\n",
      "Loss at iteration 92000: 0.5386430162164607\n",
      "Loss at iteration 93000: 0.5375243483934554\n",
      "Loss at iteration 94000: 0.5364140531694206\n",
      "Loss at iteration 95000: 0.5353120349682847\n",
      "Loss at iteration 96000: 0.5342181999257091\n",
      "Loss at iteration 97000: 0.5331324558402605\n",
      "Loss at iteration 98000: 0.5320547121265199\n",
      "Loss at iteration 99000: 0.5309848797700347\n",
      "Loss at iteration 100000: 0.5299228712840289\n",
      "Loss at iteration 101000: 0.5288686006677896\n",
      "Loss at iteration 102000: 0.52782198336665\n",
      "Loss at iteration 103000: 0.526782936233495\n",
      "Loss at iteration 104000: 0.5257513774917204\n",
      "Loss at iteration 105000: 0.5247272266995786\n",
      "Loss at iteration 106000: 0.523710404715847\n",
      "Loss at iteration 107000: 0.522700833666758\n",
      "Loss at iteration 108000: 0.5216984369141391\n",
      "Loss at iteration 109000: 0.5207031390246996\n",
      "Loss at iteration 110000: 0.5197148657404229\n",
      "Loss at iteration 111000: 0.5187335439500071\n",
      "Loss at iteration 112000: 0.5177591016613132\n",
      "Loss at iteration 113000: 0.5167914679747734\n",
      "Loss at iteration 114000: 0.5158305730577186\n",
      "Loss at iteration 115000: 0.5148763481195885\n",
      "Loss at iteration 116000: 0.5139287253879792\n",
      "Loss at iteration 117000: 0.5129876380854982\n",
      "Loss at iteration 118000: 0.5120530204073935\n",
      "Loss at iteration 119000: 0.5111248074999167\n",
      "Loss at iteration 120000: 0.5102029354393974\n",
      "Loss at iteration 121000: 0.5092873412119956\n",
      "Loss at iteration 122000: 0.5083779626941003\n",
      "Loss at iteration 123000: 0.5074747386333576\n",
      "Loss at iteration 124000: 0.5065776086302903\n",
      "Loss at iteration 125000: 0.5056865131204951\n",
      "Loss at iteration 126000: 0.5048013933573875\n",
      "Loss at iteration 127000: 0.5039221913954756\n",
      "Loss at iteration 128000: 0.503048850074141\n",
      "Loss at iteration 129000: 0.502181313001906\n",
      "Loss at iteration 130000: 0.5013195245411681\n",
      "Loss at iteration 131000: 0.5004634297933844\n",
      "Loss at iteration 132000: 0.4996129745846878\n",
      "Loss at iteration 133000: 0.4987681054519186\n",
      "Loss at iteration 134000: 0.49792876962905325\n",
      "Loss at iteration 135000: 0.49709491503402065\n",
      "Loss at iteration 136000: 0.49626649025588454\n",
      "Loss at iteration 137000: 0.49544344454238437\n",
      "Loss at iteration 138000: 0.4946257277878155\n",
      "Loss at iteration 139000: 0.4938132905212421\n",
      "Loss at iteration 140000: 0.4930060838950254\n",
      "Loss at iteration 141000: 0.4922040596736599\n",
      "Loss at iteration 142000: 0.491407170222902\n",
      "Loss at iteration 143000: 0.49061536849918497\n",
      "Loss at iteration 144000: 0.48982860803930833\n",
      "Loss at iteration 145000: 0.4890468429503897\n",
      "Loss at iteration 146000: 0.4882700279000728\n",
      "Loss at iteration 147000: 0.4874981181069835\n",
      "Loss at iteration 148000: 0.4867310693314207\n",
      "Loss at iteration 149000: 0.48596883786627854\n",
      "Loss at iteration 150000: 0.48521138052819013\n",
      "Loss at iteration 151000: 0.4844586546488853\n",
      "Loss at iteration 152000: 0.48371061806675614\n",
      "Loss at iteration 153000: 0.48296722911862167\n",
      "Loss at iteration 154000: 0.48222844663168735\n",
      "Loss at iteration 155000: 0.4814942299156902\n",
      "Loss at iteration 156000: 0.48076453875522723\n",
      "Loss at iteration 157000: 0.480039333402258\n",
      "Loss at iteration 158000: 0.4793185745687766\n",
      "Loss at iteration 159000: 0.4786022234196488\n",
      "Loss at iteration 160000: 0.4778902415656081\n",
      "Loss at iteration 161000: 0.477182591056406\n",
      "Loss at iteration 162000: 0.47647923437411205\n",
      "Loss at iteration 163000: 0.47578013442655853\n",
      "Loss at iteration 164000: 0.4750852545409259\n",
      "Loss at iteration 165000: 0.4743945584574632\n",
      "Loss at iteration 166000: 0.4737080103233432\n",
      "Loss at iteration 167000: 0.4730255746866439\n",
      "Loss at iteration 168000: 0.4723472164904543\n",
      "Loss at iteration 169000: 0.4716729010671029\n",
      "Loss at iteration 170000: 0.47100259413250284\n",
      "Loss at iteration 171000: 0.47033626178060844\n",
      "Loss at iteration 172000: 0.46967387047798737\n",
      "Loss at iteration 173000: 0.4690153870584969\n",
      "Loss at iteration 174000: 0.46836077871806514\n",
      "Loss at iteration 175000: 0.46771001300957604\n",
      "Loss at iteration 176000: 0.46706305783785207\n",
      "Loss at iteration 177000: 0.466419881454732\n",
      "Loss at iteration 178000: 0.46578045245424515\n",
      "Loss at iteration 179000: 0.4651447397678735\n",
      "Loss at iteration 180000: 0.46451271265990685\n",
      "Loss at iteration 181000: 0.4638843407228795\n",
      "Loss at iteration 182000: 0.46325959387309495\n",
      "Loss at iteration 183000: 0.46263844234623097\n",
      "Loss at iteration 184000: 0.46202085669302445\n",
      "Loss at iteration 185000: 0.46140680777503496\n",
      "Loss at iteration 186000: 0.46079626676048374\n",
      "Loss at iteration 187000: 0.46018920512016565\n",
      "Loss at iteration 188000: 0.4595855946234371\n",
      "Loss at iteration 189000: 0.4589854073342686\n",
      "Loss at iteration 190000: 0.45838861560737154\n",
      "Loss at iteration 191000: 0.45779519208438796\n",
      "Loss at iteration 192000: 0.4572051096901487\n",
      "Loss at iteration 193000: 0.45661834162899334\n",
      "Loss at iteration 194000: 0.45603486138115384\n",
      "Loss at iteration 195000: 0.4554546426991991\n",
      "Loss at iteration 196000: 0.4548776596045373\n",
      "Loss at iteration 197000: 0.45430388638397784\n",
      "Loss at iteration 198000: 0.4537332975863517\n",
      "Loss at iteration 199000: 0.4531658680191832\n",
      "Loss at iteration 200000: 0.45260157274542157\n",
      "Loss at iteration 201000: 0.45204038708021893\n",
      "Loss at iteration 202000: 0.4514822865877664\n",
      "Loss at iteration 203000: 0.45092724707817783\n",
      "Loss at iteration 204000: 0.45037524460442263\n",
      "Loss at iteration 205000: 0.44982625545930993\n",
      "Loss at iteration 206000: 0.4492802561725175\n",
      "Loss at iteration 207000: 0.4487372235076707\n",
      "Loss at iteration 208000: 0.44819713445946235\n",
      "Loss at iteration 209000: 0.44765996625082094\n",
      "Loss at iteration 210000: 0.44712569633012106\n",
      "Loss at iteration 211000: 0.4465943023684372\n",
      "Loss at iteration 212000: 0.44606576225683947\n",
      "Loss at iteration 213000: 0.44554005410373027\n",
      "Loss at iteration 214000: 0.4450171562322199\n",
      "Loss at iteration 215000: 0.44449704717754507\n",
      "Loss at iteration 216000: 0.44397970568452216\n",
      "Loss at iteration 217000: 0.4434651107050405\n",
      "Loss at iteration 218000: 0.4429532413955939\n",
      "Loss at iteration 219000: 0.4424440771148456\n",
      "Loss at iteration 220000: 0.44193759742123134\n",
      "Loss at iteration 221000: 0.44143378207059664\n",
      "Loss at iteration 222000: 0.44093261101386966\n",
      "Loss at iteration 223000: 0.4404340643947647\n",
      "Loss at iteration 224000: 0.439938122547524\n",
      "Loss at iteration 225000: 0.4394447659946863\n",
      "Loss at iteration 226000: 0.4389539754448915\n",
      "Loss at iteration 227000: 0.4384657317907147\n",
      "Loss at iteration 228000: 0.43798001610653137\n",
      "Loss at iteration 229000: 0.43749680964641313\n",
      "Loss at iteration 230000: 0.43701609384205303\n",
      "Loss at iteration 231000: 0.43653785030071957\n",
      "Loss at iteration 232000: 0.43606206080323917\n",
      "Loss at iteration 233000: 0.435588707302008\n",
      "Loss at iteration 234000: 0.4351177719190294\n",
      "Loss at iteration 235000: 0.434649236943981\n",
      "Loss at iteration 236000: 0.43418308483230483\n",
      "Loss at iteration 237000: 0.4337192982033283\n",
      "Loss at iteration 238000: 0.4332578598384064\n",
      "Loss at iteration 239000: 0.4327987526790923\n",
      "Loss at iteration 240000: 0.4323419598253322\n",
      "Loss at iteration 241000: 0.43188746453368193\n",
      "Loss at iteration 242000: 0.43143525021555273\n",
      "Loss at iteration 243000: 0.43098530043547556\n",
      "Loss at iteration 244000: 0.4305375989093916\n",
      "Loss at iteration 245000: 0.43009212950296394\n",
      "Loss at iteration 246000: 0.4296488762299152\n",
      "Loss at iteration 247000: 0.4292078232503811\n",
      "Loss at iteration 248000: 0.4287689548692918\n",
      "Loss at iteration 249000: 0.42833225553477156\n",
      "Loss at iteration 250000: 0.42789770983655995\n",
      "Loss at iteration 251000: 0.4274653025044538\n",
      "Loss at iteration 252000: 0.42703501840676944\n",
      "Loss at iteration 253000: 0.4266068425488257\n",
      "Loss at iteration 254000: 0.42618076007144595\n",
      "Loss at iteration 255000: 0.4257567562494792\n",
      "Loss at iteration 256000: 0.4253348164903414\n",
      "Loss at iteration 257000: 0.4249149263325747\n",
      "Loss at iteration 258000: 0.4244970714444258\n",
      "Loss at iteration 259000: 0.4240812376224417\n",
      "Loss at iteration 260000: 0.4236674107900846\n",
      "Loss at iteration 261000: 0.42325557699636346\n",
      "Loss at iteration 262000: 0.4228457224144835\n",
      "Loss at iteration 263000: 0.42243783334051294\n",
      "Loss at iteration 264000: 0.42203189619206544\n",
      "Loss at iteration 265000: 0.421627897507001\n",
      "Loss at iteration 266000: 0.42122582394214275\n",
      "Loss at iteration 267000: 0.4208256622720077\n",
      "Loss at iteration 268000: 0.42042739938755713\n",
      "Loss at iteration 269000: 0.42003102229495937\n",
      "Loss at iteration 270000: 0.4196365181143697\n",
      "Loss at iteration 271000: 0.41924387407872543\n",
      "Loss at iteration 272000: 0.4188530775325541\n",
      "Loss at iteration 273000: 0.41846411593080013\n",
      "Loss at iteration 274000: 0.41807697683766126\n",
      "Loss at iteration 275000: 0.4176916479254439\n",
      "Loss at iteration 276000: 0.41730811697342896\n",
      "Loss at iteration 277000: 0.41692637186675374\n",
      "Loss at iteration 278000: 0.41654640059530745\n",
      "Loss at iteration 279000: 0.4161681912526385\n",
      "Loss at iteration 280000: 0.41579173203487674\n",
      "Loss at iteration 281000: 0.4154170112396691\n",
      "Loss at iteration 282000: 0.415044017265126\n",
      "Loss at iteration 283000: 0.4146727386087835\n",
      "Loss at iteration 284000: 0.41430316386657556\n",
      "Loss at iteration 285000: 0.4139352817318192\n",
      "Loss at iteration 286000: 0.4135690809942137\n",
      "Loss at iteration 287000: 0.41320455053884897\n",
      "Loss at iteration 288000: 0.4128416793452282\n",
      "Loss at iteration 289000: 0.41248045648630105\n",
      "Loss at iteration 290000: 0.41212087112750795\n",
      "Loss at iteration 291000: 0.4117629125258376\n",
      "Loss at iteration 292000: 0.411406570028894\n",
      "Loss at iteration 293000: 0.4110518330739751\n",
      "Loss at iteration 294000: 0.4106986911871624\n",
      "Loss at iteration 295000: 0.41034713398242123\n",
      "Loss at iteration 296000: 0.40999715116071195\n",
      "Loss at iteration 297000: 0.40964873250911216\n",
      "Loss at iteration 298000: 0.40930186789994616\n",
      "Loss at iteration 299000: 0.40895654728992986\n",
      "Loss at iteration 300000: 0.4086127607193211\n",
      "Loss at iteration 301000: 0.4082704983110818\n",
      "Loss at iteration 302000: 0.4079297502700509\n",
      "Loss at iteration 303000: 0.4075905068821239\n",
      "Loss at iteration 304000: 0.40725275851344617\n",
      "Loss at iteration 305000: 0.40691649560961074\n",
      "Loss at iteration 306000: 0.40658170869486976\n",
      "Loss at iteration 307000: 0.406248388371353\n",
      "Loss at iteration 308000: 0.4059165253182955\n",
      "Loss at iteration 309000: 0.40558611029127384\n",
      "Loss at iteration 310000: 0.40525713412145226\n",
      "Loss at iteration 311000: 0.404929587714837\n",
      "Loss at iteration 312000: 0.40460346205153835\n",
      "Loss at iteration 313000: 0.4042787481850425\n",
      "Loss at iteration 314000: 0.403955437241491\n",
      "Loss at iteration 315000: 0.40363352041896844\n",
      "Loss at iteration 316000: 0.4033129889867978\n",
      "Loss at iteration 317000: 0.40299383428484514\n",
      "Loss at iteration 318000: 0.40267604772283105\n",
      "Loss at iteration 319000: 0.4023596207796506\n",
      "Loss at iteration 320000: 0.40204454500270065\n",
      "Loss at iteration 321000: 0.40173081200721367\n",
      "Loss at iteration 322000: 0.4014184134756018\n",
      "Loss at iteration 323000: 0.4011073411568052\n",
      "Loss at iteration 324000: 0.4007975868656509\n",
      "Loss at iteration 325000: 0.40048914248221545\n",
      "Loss at iteration 326000: 0.40018199995119763\n",
      "Loss at iteration 327000: 0.39987615128129655\n",
      "Loss at iteration 328000: 0.399571588544597\n",
      "Loss at iteration 329000: 0.3992683038759619\n",
      "Loss at iteration 330000: 0.39896628947243185\n",
      "Loss at iteration 331000: 0.39866553759263057\n",
      "Loss at iteration 332000: 0.3983660405561767\n",
      "Loss at iteration 333000: 0.3980677907431042\n",
      "Loss at iteration 334000: 0.39777078059328613\n",
      "Loss at iteration 335000: 0.39747500260586716\n",
      "Loss at iteration 336000: 0.3971804493387021\n",
      "Loss at iteration 337000: 0.39688711340779925\n",
      "Loss at iteration 338000: 0.3965949874867706\n",
      "Loss at iteration 339000: 0.3963040643062881\n",
      "Loss at iteration 340000: 0.3960143366535474\n",
      "Loss at iteration 341000: 0.39572579737173386\n",
      "Loss at iteration 342000: 0.3954384393594984\n",
      "Loss at iteration 343000: 0.3951522555704361\n",
      "Loss at iteration 344000: 0.3948672390125725\n",
      "Loss at iteration 345000: 0.3945833827478535\n",
      "Loss at iteration 346000: 0.3943006798916431\n",
      "Loss at iteration 347000: 0.3940191236122246\n",
      "Loss at iteration 348000: 0.3937387071303088\n",
      "Loss at iteration 349000: 0.39345942371854536\n",
      "Loss at iteration 350000: 0.39318126670104275\n",
      "Loss at iteration 351000: 0.39290422945288955\n",
      "Loss at iteration 352000: 0.39262830539968463\n",
      "Loss at iteration 353000: 0.3923534880170686\n",
      "Loss at iteration 354000: 0.3920797708302641\n",
      "Loss at iteration 355000: 0.39180714741361794\n",
      "Loss at iteration 356000: 0.3915356113901505\n",
      "Loss at iteration 357000: 0.3912651564311072\n",
      "Loss at iteration 358000: 0.390995776255518\n",
      "Loss at iteration 359000: 0.3907274646297597\n",
      "Loss at iteration 360000: 0.3904602153671219\n",
      "Loss at iteration 361000: 0.3901940223273807\n",
      "Loss at iteration 362000: 0.38992887941637355\n",
      "Loss at iteration 363000: 0.3896647805855815\n",
      "Loss at iteration 364000: 0.38940171983171323\n",
      "Loss at iteration 365000: 0.3891396911962961\n",
      "Loss at iteration 366000: 0.3888786887652689\n",
      "Loss at iteration 367000: 0.3886187066685809\n",
      "Loss at iteration 368000: 0.38835973907979454\n",
      "Loss at iteration 369000: 0.38810178021569186\n",
      "Loss at iteration 370000: 0.387844824335885\n",
      "Loss at iteration 371000: 0.38758886574243234\n",
      "Loss at iteration 372000: 0.3873338987794557\n",
      "Loss at iteration 373000: 0.38707991783276513\n",
      "Loss at iteration 374000: 0.3868269173294846\n",
      "Loss at iteration 375000: 0.3865748917376833\n",
      "Loss at iteration 376000: 0.3863238355660095\n",
      "Loss at iteration 377000: 0.3860737433633304\n",
      "Loss at iteration 378000: 0.3858246097183728\n",
      "Loss at iteration 379000: 0.38557642925937036\n",
      "Loss at iteration 380000: 0.3853291966537114\n",
      "Loss at iteration 381000: 0.385082906607594\n",
      "Loss at iteration 382000: 0.3848375538656811\n",
      "Loss at iteration 383000: 0.38459313321076194\n",
      "Loss at iteration 384000: 0.38434963946341466\n",
      "Loss at iteration 385000: 0.38410706748167556\n",
      "Loss at iteration 386000: 0.38386541216070774\n",
      "Loss at iteration 387000: 0.38362466843247556\n",
      "Loss at iteration 388000: 0.38338483126542255\n",
      "Loss at iteration 389000: 0.38314589566415164\n",
      "Loss at iteration 390000: 0.3829078566691094\n",
      "Loss at iteration 391000: 0.38267070935627273\n",
      "Loss at iteration 392000: 0.38243444883683997\n",
      "Loss at iteration 393000: 0.3821990702569231\n",
      "Loss at iteration 394000: 0.38196456879724533\n",
      "Loss at iteration 395000: 0.38173093967284044\n",
      "Loss at iteration 396000: 0.38149817813275566\n",
      "Loss at iteration 397000: 0.381266279459757\n",
      "Loss at iteration 398000: 0.3810352389700386\n",
      "Loss at iteration 399000: 0.3808050520129328\n",
      "Loss at iteration 400000: 0.3805757139706266\n",
      "Loss at iteration 401000: 0.38034722025787765\n",
      "Loss at iteration 402000: 0.3801195663217355\n",
      "Loss at iteration 403000: 0.37989274764126363\n",
      "Loss at iteration 404000: 0.3796667597272649\n",
      "Loss at iteration 405000: 0.37944159812201256\n",
      "Loss at iteration 406000: 0.379217258398979\n",
      "Loss at iteration 407000: 0.3789937361625694\n",
      "Loss at iteration 408000: 0.3787710270478609\n",
      "Loss at iteration 409000: 0.3785491267203392\n",
      "Loss at iteration 410000: 0.37832803087564126\n",
      "Loss at iteration 411000: 0.3781077352393013\n",
      "Loss at iteration 412000: 0.3778882355664945\n",
      "Loss at iteration 413000: 0.3776695276417882\n",
      "Loss at iteration 414000: 0.37745160727889443\n",
      "Loss at iteration 415000: 0.377234470320422\n",
      "Loss at iteration 416000: 0.3770181126376356\n",
      "Loss at iteration 417000: 0.37680253013021364\n",
      "Loss at iteration 418000: 0.37658771872600977\n",
      "Loss at iteration 419000: 0.3763736743808167\n",
      "Loss at iteration 420000: 0.3761603930781326\n",
      "Loss at iteration 421000: 0.37594787082892994\n",
      "Loss at iteration 422000: 0.37573610367142435\n",
      "Loss at iteration 423000: 0.37552508767084924\n",
      "Loss at iteration 424000: 0.3753148189192311\n",
      "Loss at iteration 425000: 0.37510529353516586\n",
      "Loss at iteration 426000: 0.37489650766359883\n",
      "Loss at iteration 427000: 0.3746884574756062\n",
      "Loss at iteration 428000: 0.37448113916817877\n",
      "Loss at iteration 429000: 0.374274548964007\n",
      "Loss at iteration 430000: 0.37406868311127106\n",
      "Loss at iteration 431000: 0.37386353788342885\n",
      "Loss at iteration 432000: 0.3736591095790077\n",
      "Loss at iteration 433000: 0.3734553945214007\n",
      "Loss at iteration 434000: 0.37325238905865976\n",
      "Loss at iteration 435000: 0.3730500895632958\n",
      "Loss at iteration 436000: 0.3728484924320779\n",
      "Loss at iteration 437000: 0.3726475940858342\n",
      "Loss at iteration 438000: 0.3724473909692563\n",
      "Loss at iteration 439000: 0.3722478795507049\n",
      "Loss at iteration 440000: 0.37204905632201735\n",
      "Loss at iteration 441000: 0.37185091779831597\n",
      "Loss at iteration 442000: 0.3716534605178204\n",
      "Loss at iteration 443000: 0.37145668104165946\n",
      "Loss at iteration 444000: 0.3712605759536853\n",
      "Loss at iteration 445000: 0.3710651418602923\n",
      "Loss at iteration 446000: 0.3708703753902318\n",
      "Loss at iteration 447000: 0.3706762731944343\n",
      "Loss at iteration 448000: 0.3704828319458299\n",
      "Loss at iteration 449000: 0.3702900483391717\n",
      "Loss at iteration 450000: 0.37009791909086137\n",
      "Loss at iteration 451000: 0.36990644093877495\n",
      "Loss at iteration 452000: 0.3697156106420908\n",
      "Loss at iteration 453000: 0.36952542498112\n",
      "Loss at iteration 454000: 0.3693358807571377\n",
      "Loss at iteration 455000: 0.3691469747922154\n",
      "Loss at iteration 456000: 0.3689587039290561\n",
      "Loss at iteration 457000: 0.36877106503083007\n",
      "Loss at iteration 458000: 0.3685840549810124\n",
      "Loss at iteration 459000: 0.36839767068322204\n",
      "Loss at iteration 460000: 0.36821190906106277\n",
      "Loss at iteration 461000: 0.36802676705796417\n",
      "Loss at iteration 462000: 0.36784224163702733\n",
      "Loss at iteration 463000: 0.36765832978086677\n",
      "Loss at iteration 464000: 0.3674750284914596\n",
      "Loss at iteration 465000: 0.36729233478999185\n",
      "Loss at iteration 466000: 0.367110245716708\n",
      "Loss at iteration 467000: 0.36692875833076116\n",
      "Loss at iteration 468000: 0.3667478697100667\n",
      "Loss at iteration 469000: 0.36656757695115333\n",
      "Loss at iteration 470000: 0.3663878771690197\n",
      "Loss at iteration 471000: 0.3662087674969888\n",
      "Loss at iteration 472000: 0.3660302450865668\n",
      "Loss at iteration 473000: 0.36585230710730005\n",
      "Loss at iteration 474000: 0.3656749507466371\n",
      "Loss at iteration 475000: 0.3654981732097881\n",
      "Loss at iteration 476000: 0.3653219717195879\n",
      "Loss at iteration 477000: 0.3651463435163598\n",
      "Loss at iteration 478000: 0.3649712858577811\n",
      "Loss at iteration 479000: 0.36479679601874815\n",
      "Loss at iteration 480000: 0.36462287129124377\n",
      "Loss at iteration 481000: 0.364449508984207\n",
      "Loss at iteration 482000: 0.3642767064234023\n",
      "Loss at iteration 483000: 0.36410446095128984\n",
      "Loss at iteration 484000: 0.3639327699268989\n",
      "Loss at iteration 485000: 0.3637616307256998\n",
      "Loss at iteration 486000: 0.3635910407394795\n",
      "Loss at iteration 487000: 0.3634209973762162\n",
      "Loss at iteration 488000: 0.3632514980599564\n",
      "Loss at iteration 489000: 0.363082540230693\n",
      "Loss at iteration 490000: 0.36291412134424333\n",
      "Loss at iteration 491000: 0.3627462388721293\n",
      "Loss at iteration 492000: 0.3625788903014591\n",
      "Loss at iteration 493000: 0.36241207313480894\n",
      "Loss at iteration 494000: 0.3622457848901057\n",
      "Loss at iteration 495000: 0.36208002310051096\n",
      "Loss at iteration 496000: 0.3619147853143069\n",
      "Loss at iteration 497000: 0.36175006909478263\n",
      "Loss at iteration 498000: 0.3615858720201203\n",
      "Loss at iteration 499000: 0.3614221916832842\n"
     ]
    }
   ],
   "source": [
    "penguin_model.fit(x_train, y_train, intial_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Test accuracy with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 69.84126984126983%\n"
     ]
    }
   ],
   "source": [
    "n_correct = 0\n",
    "x_test_copy = x_test.values # Converting to np arrays\n",
    "y_test_copy = y_test.values\n",
    "\n",
    "for i in range(y_test_copy.shape[0]):\n",
    "    y = penguin_model.predict(x_test_copy[i])\n",
    "    if y == y_test_copy[i]:\n",
    "        n_correct += 1\n",
    "\n",
    "accuracy = n_correct/y_test_copy.shape[0]\n",
    "\n",
    "print(f\"Accuracy is {accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Print Loss Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(len(penguin_model.loss)), penguin_model.loss)\n",
    "plt.title(\"Graph of Loss over iteration\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFERENCES\n",
    "\n",
    "https://wiki.python.org/moin/UsingPickle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
